{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Complete Training (IL + RL)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random, vmap, device_get\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from socialjym.envs.socialnav import SocialNav\n",
    "from socialjym.policies.cadrl import CADRL\n",
    "from socialjym.policies.sarl import SARL\n",
    "from socialjym.utils.replay_buffers.uniform_vnet_replay_buffer import UniformVNetReplayBuffer\n",
    "from socialjym.utils.rollouts.deep_vnet_rollouts import deep_vnet_rl_rollout, deep_vnet_il_rollout\n",
    "from socialjym.utils.aux_functions import epsilon_scaling_decay, plot_state, plot_trajectory, test_k_trials, save_policy_params\n",
    "from socialjym.utils.rewards.socialnav_rewards.reward1 import Reward1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_hyperparams = {\n",
    "    'random_seed': 0,\n",
    "    'policy_name': 'cadrl', # 'cadrl' or 'sarl'\n",
    "    'n_humans': 1,  # CADRL uses 1, SARL uses 5\n",
    "    'kinematics': 'holonomic', # 'holonomic' or 'unicycle'\n",
    "    'il_training_episodes': 3_000,\n",
    "    'il_learning_rate': 0.01,\n",
    "    'il_num_epochs': 50, # Number of epochs to train the model after ending IL\n",
    "    'rl_training_episodes': 10_000,\n",
    "    'rl_learning_rate': 0.001,\n",
    "    'rl_num_batches': 100, # Number of batches to train the model after each RL episode\n",
    "    'batch_size': 100, # Number of experiences to sample from the replay buffer for each model update\n",
    "    'epsilon_start': 0.5,\n",
    "    'epsilon_end': 0.1,\n",
    "    'epsilon_decay': 4_000,\n",
    "    'buffer_size': 100_000, # Maximum number of experiences to store in the replay buffer (after exceeding this limit, the oldest experiences are overwritten with new ones)\n",
    "    'target_update_interval': 50, # Number of episodes to wait before updating the target network for RL (the one used to compute the target state values)\n",
    "    'humans_policy': 'sfm',\n",
    "    'scenario': 'hybrid_scenario',\n",
    "    'hybrid_scenario_subset': jnp.array([0,1], np.int32), # Subset of the hybrid scenarios to use for training\n",
    "    'reward_function': 'socialnav_reward1',\n",
    "    'custom_episodes': True, # If True, the episodes are loaded from a predefined set\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the reward and environment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize reward function\n",
    "if training_hyperparams['reward_function'] == 'socialnav_reward1': \n",
    "    reward_function = Reward1(kinematics=training_hyperparams['kinematics'])\n",
    "else:\n",
    "    raise ValueError(f\"{training_hyperparams['reward_function']} is not a valid reward function\")\n",
    "\n",
    "# Environment parameters\n",
    "env_params = {\n",
    "    'robot_radius': 0.3,\n",
    "    'n_humans': training_hyperparams['n_humans'],\n",
    "    'robot_dt': 0.25,\n",
    "    'humans_dt': 0.01,\n",
    "    'robot_visible': False,\n",
    "    'scenario': training_hyperparams['scenario'],\n",
    "    'hybrid_scenario_subset': training_hyperparams['hybrid_scenario_subset'],\n",
    "    'humans_policy': training_hyperparams['humans_policy'],\n",
    "    'circle_radius': 7,\n",
    "    'reward_function': reward_function,\n",
    "    'kinematics': training_hyperparams['kinematics'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize environment, robot policy and replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "env = SocialNav(**env_params)\n",
    "\n",
    "# Initialize robot policy and vnet params\n",
    "if training_hyperparams['policy_name'] == \"cadrl\": \n",
    "    policy = CADRL(env.reward_function, dt=env_params['robot_dt'])\n",
    "    initial_vnet_params = policy.model.init(random.key(training_hyperparams['random_seed']), jnp.zeros((policy.vnet_input_size,)))\n",
    "elif training_hyperparams['policy_name'] == \"sarl\":\n",
    "    policy = SARL(env.reward_function, dt=env_params['robot_dt'])\n",
    "    initial_vnet_params = policy.model.init(random.key(training_hyperparams['random_seed']), jnp.zeros((env_params['n_humans'], policy.vnet_input_size)))\n",
    "else: raise ValueError(f\"{training_hyperparams['policy_name']} is not a valid policy name\")\n",
    "\n",
    "# Initialize replay buffer\n",
    "replay_buffer = UniformVNetReplayBuffer(training_hyperparams['buffer_size'], training_hyperparams['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Imitation Learning</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the optimizer and the buffer state dictionary (where experiences will be stored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize IL optimizer\n",
    "optimizer = optax.sgd(learning_rate=training_hyperparams['il_learning_rate'], momentum=0.9)\n",
    "\n",
    "# Initialize buffer state\n",
    "buffer_state = {\n",
    "    'vnet_inputs': jnp.empty((training_hyperparams['buffer_size'], env.n_humans, policy.vnet_input_size)),\n",
    "    'targets': jnp.empty((training_hyperparams['buffer_size'],1)),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set all the parameters for the imitation learning rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize custom episodes path\n",
    "if training_hyperparams['custom_episodes']:\n",
    "    il_custom_episodes_path = os.path.join(os.path.expanduser(\"~\"),f\"Repos/social-jym/custom_episodes/il_{training_hyperparams['scenario']}_{training_hyperparams['n_humans']}_humans.pkl\")\n",
    "else:\n",
    "    il_custom_episodes_path = None\n",
    "\n",
    "# Initialize IL rollout params\n",
    "il_rollout_params = {\n",
    "    'initial_vnet_params': initial_vnet_params,\n",
    "    'train_episodes': training_hyperparams['il_training_episodes'],\n",
    "    'random_seed': training_hyperparams['random_seed'],\n",
    "    'optimizer': optimizer,\n",
    "    'buffer_state': buffer_state,\n",
    "    'current_buffer_size': 0,\n",
    "    'policy': policy,\n",
    "    'env': env,\n",
    "    'replay_buffer': replay_buffer,\n",
    "    'buffer_size': training_hyperparams['buffer_size'],\n",
    "    'num_epochs': training_hyperparams['il_num_epochs'],\n",
    "    'batch_size': training_hyperparams['batch_size'],\n",
    "    'custom_episodes': il_custom_episodes_path\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During imitation learning, the robot will move using the same policy used by humans. Let's start the rollout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "il_out = deep_vnet_il_rollout(**il_rollout_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we save the parameters we are interested in from the rollout output and we plot the discounted return over the IL training episodes and the loss over the optimization epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the IL model parameters, buffer state, and keys\n",
    "il_model_params = il_out['model_params']\n",
    "buffer_state = il_out['buffer_state']\n",
    "current_buffer_size = il_out['current_buffer_size']\n",
    "\n",
    "# Plot the losses and returns\n",
    "window = 100\n",
    "figure, ax = plt.subplots(figsize=(10,10))\n",
    "ax.set(xlabel='Episodes', ylabel='Return', title='Return moving average over {} episodes'.format(window))\n",
    "ax.plot(np.arange(len(il_out['returns'])-(window-1))+window, jnp.convolve(il_out['returns'], jnp.ones(window,), 'valid') / window)\n",
    "plt.show()\n",
    "figure, ax = plt.subplots(figsize=(10,10))\n",
    "ax.set(xlabel='Episodes', ylabel='Loss', title='Loss over {} epochs'.format(len(il_out['losses'])))\n",
    "ax.plot(np.arange(len(il_out['losses'])), il_out['losses'])\n",
    "plt.show()\n",
    "\n",
    "# Print loss of the 10 last epochs\n",
    "print(f\"Loss of the last 10 epochs: \\n{il_out['losses'][-10:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the IL trained agent on 1000 unseen trials. The robot is still NOT visible by humans here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_k_trials(\n",
    "    1000, \n",
    "    training_hyperparams['random_seed'] + training_hyperparams['il_training_episodes'], \n",
    "    env, \n",
    "    policy, \n",
    "    il_model_params, \n",
    "    reward_function.time_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Reinforcement Learning</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the optimizer and the next rollout parameters. We should start from the model parameters compute after IL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RL optimizer\n",
    "optimizer = optax.sgd(learning_rate=training_hyperparams['rl_learning_rate'], momentum=0.9)\n",
    "\n",
    "# Initialize custom episodes path\n",
    "if training_hyperparams['custom_episodes']:\n",
    "    rl_custom_episodes_path = os.path.join(os.path.expanduser(\"~\"),f\"Repos/social-jym/custom_episodes/rl_{training_hyperparams['scenario']}_{training_hyperparams['n_humans']}_humans.pkl\")\n",
    "else:\n",
    "    rl_custom_episodes_path = None\n",
    "\n",
    "# Initialize RL rollout params\n",
    "rl_rollout_params = {\n",
    "    'initial_vnet_params': il_model_params,\n",
    "    'train_episodes': training_hyperparams['rl_training_episodes'],\n",
    "    'random_seed': training_hyperparams['random_seed'] + training_hyperparams['il_training_episodes'],\n",
    "    'model': policy.model,\n",
    "    'optimizer': optimizer,\n",
    "    'buffer_state': buffer_state,\n",
    "    'current_buffer_size': current_buffer_size,\n",
    "    'policy': policy,\n",
    "    'env': env,\n",
    "    'replay_buffer': replay_buffer,\n",
    "    'buffer_size': training_hyperparams['buffer_size'],\n",
    "    'num_batches': training_hyperparams['rl_num_batches'],\n",
    "    'epsilon_decay_fn': epsilon_scaling_decay,\n",
    "    'epsilon_start': training_hyperparams['epsilon_start'],\n",
    "    'epsilon_end': training_hyperparams['epsilon_end'],\n",
    "    'decay_rate': training_hyperparams['epsilon_decay'],\n",
    "    'target_update_interval': training_hyperparams['target_update_interval'],\n",
    "    'custom_episodes': rl_custom_episodes_path,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the RL rollout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_out = deep_vnet_rl_rollout(**rl_rollout_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the final model parameters and plot discounted return and loss over the RL training episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model parameters and keys\n",
    "final_model_params = rl_out['model_params']\n",
    "\n",
    "figure, ax = plt.subplots(figsize=(10,10))\n",
    "window = 500\n",
    "ax.plot(np.arange(len(rl_out['losses'])-(window-1))+window, jnp.convolve(rl_out['losses'], jnp.ones(window,), 'valid') / window)\n",
    "ax.set(xlabel='Episodes', ylabel='Loss', title='Loss moving average over {} episodes'.format(window))\n",
    "plt.show()\n",
    "figure, ax = plt.subplots(figsize=(10,10))\n",
    "ax.set(xlabel='Episodes', ylabel='Return', title='Return moving average over {} episodes'.format(window))\n",
    "ax.plot(np.arange(len(rl_out['returns'])-(window-1))+window, jnp.convolve(rl_out['returns'], jnp.ones(window,), 'valid') / window)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the RL trained agent in three environments, with 1, 5 and 10 humans. In all environments the robot is visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env5_params = {\n",
    "    'robot_radius': 0.3,\n",
    "    'n_humans': 5,\n",
    "    'robot_dt': 0.25,\n",
    "    'humans_dt': 0.01,\n",
    "    'robot_visible': True,\n",
    "    'scenario': training_hyperparams['scenario'],\n",
    "    'hybrid_scenario_subset': training_hyperparams['hybrid_scenario_subset'],\n",
    "    'humans_policy': training_hyperparams['humans_policy'],\n",
    "    'reward_function': reward_function,\n",
    "    'kinematics': training_hyperparams['kinematics'],\n",
    "}\n",
    "env5 = SocialNav(**env5_params)\n",
    "env10_params = {\n",
    "    'robot_radius': 0.3,\n",
    "    'n_humans': 10,\n",
    "    'robot_dt': 0.25,\n",
    "    'humans_dt': 0.01,\n",
    "    'robot_visible': True,\n",
    "    'scenario': training_hyperparams['scenario'],\n",
    "    'hybrid_scenario_subset': training_hyperparams['hybrid_scenario_subset'],\n",
    "    'humans_policy': training_hyperparams['humans_policy'],\n",
    "    'reward_function': reward_function,\n",
    "    'kinematics': training_hyperparams['kinematics'],\n",
    "}\n",
    "env10 = SocialNav(**env10_params)\n",
    "env15_params = {\n",
    "    'robot_radius': 0.3,\n",
    "    'n_humans': 15,\n",
    "    'robot_dt': 0.25,\n",
    "    'humans_dt': 0.01,\n",
    "    'robot_visible': True,\n",
    "    'scenario': training_hyperparams['scenario'],\n",
    "    'hybrid_scenario_subset': training_hyperparams['hybrid_scenario_subset'],\n",
    "    'humans_policy': training_hyperparams['humans_policy'],\n",
    "    'reward_function': reward_function,\n",
    "    'kinematics': training_hyperparams['kinematics'],\n",
    "}\n",
    "env15 = SocialNav(**env15_params)\n",
    "## Execute tests\n",
    "test_k_trials(\n",
    "    1000, \n",
    "    training_hyperparams['random_seed'] + training_hyperparams['il_training_episodes'] + training_hyperparams['rl_training_episodes'], \n",
    "    env5, \n",
    "    policy, \n",
    "    final_model_params, \n",
    "    reward_function.time_limit)\n",
    "test_k_trials(\n",
    "    1000, \n",
    "    0, # The seed does not matter since the environment has more humans than the training one\n",
    "    env10, \n",
    "    policy, \n",
    "    final_model_params, \n",
    "    reward_function.time_limit)\n",
    "test_k_trials(\n",
    "    1000, \n",
    "    0, # The seed does not matter since the environment has more humans than the training one\n",
    "    env15, \n",
    "    policy, \n",
    "    final_model_params, \n",
    "    reward_function.time_limit) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate some episodes using the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 5\n",
    "env = SocialNav(**env_params)\n",
    "# Simulate some episodes\n",
    "episode_simulation_times = np.empty((n_episodes,))\n",
    "for i in range(n_episodes):\n",
    "    policy_key, reset_key = vmap(random.PRNGKey)(jnp.zeros(2, dtype=int) + i)\n",
    "    episode_start_time = time.time()\n",
    "    state, reset_key, obs, info, outcome = env.reset(reset_key)\n",
    "    all_states = np.array([state])\n",
    "    while outcome[\"nothing\"]:\n",
    "        # action = jnp.array([0.,1.]) # Move north\n",
    "        action, policy_key, _ = policy.act(policy_key, obs, info, final_model_params, 0.)\n",
    "        state, obs, info, reward, outcome = env.step(state,info,action,test=True) \n",
    "        all_states = np.vstack((all_states, [state]))\n",
    "    episode_simulation_times[i] = round(time.time() - episode_start_time,2)\n",
    "    all_states = device_get(all_states) # Transfer data from GPU to CPU for plotting\n",
    "    print(f\"Episode {i} ended - Execution time {episode_simulation_times[i]} seconds - Plotting trajectory...\")\n",
    "    ## Plot episode trajectory\n",
    "    figure, ax = plt.subplots(figsize=(10,10))\n",
    "    ax.axis('equal')\n",
    "    plot_trajectory(ax, all_states, info['humans_goal'], info['robot_goal'])\n",
    "    for k in range(0,len(all_states),int(3/env_params['robot_dt'])):\n",
    "        plot_state(ax, k*env_params['robot_dt'], all_states[k], env_params['humans_policy'], info['current_scenario'], info[\"humans_parameters\"][:,0], env.robot_radius, kinematics=env_params['kinematics'])\n",
    "    # plot last state\n",
    "    plot_state(ax, (len(all_states)-1)*env_params['robot_dt'], all_states[len(all_states)-1], env_params['humans_policy'], info['current_scenario'], info[\"humans_parameters\"][:,0], env.robot_radius, kinematics=env_params['kinematics'])\n",
    "    plt.show()\n",
    "# Print simulation times\n",
    "print(f\"Average time per episode: {round(np.mean(episode_simulation_times),2)} seconds\")\n",
    "print(f\"Total time for {n_episodes} episodes: {round(np.sum(episode_simulation_times),2)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained policy parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_policy_params(\n",
    "    training_hyperparams['policy_name'], \n",
    "    final_model_params, \n",
    "    env.get_parameters(), \n",
    "    reward_function.get_parameters(), \n",
    "    training_hyperparams, \n",
    "    os.path.join(os.path.expanduser(\"~\"),\"Repos/social-jym/trained_policies/socialjym_policies/\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socialjym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
