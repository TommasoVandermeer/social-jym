{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CADRL Complete Training (IL + RL)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tommaso/PyVenvs/socialjym/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from jax import random, device_get\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from socialjym.envs.socialnav import SocialNav\n",
    "from socialjym.policies.sarl import SARL\n",
    "from socialjym.utils.replay_buffers.uniform_vnet_replay_buffer import UniformVNetReplayBuffer\n",
    "from socialjym.utils.rollouts.deep_vnet_rollouts import deep_vnet_rl_rollout, deep_vnet_il_rollout\n",
    "from socialjym.utils.aux_functions import epsilon_scaling_decay, plot_state, plot_trajectory, test_k_trials\n",
    "from socialjym.utils.rewards.reward1 import generate_reward_done_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1\n",
    "il_training_episodes = 3_000\n",
    "il_learning_rate = 0.001 # For SARL this has to be lower than for CADRL\n",
    "il_num_epochs = 50 # Number of epochs to train the model after ending IL\n",
    "rl_training_episodes = 10_000\n",
    "rl_learning_rate = 0.001\n",
    "rl_num_batches = 100 # Number of batches to train the model after each RL episode\n",
    "batch_size = 100 # Number of experiences to sample from the replay buffer for each model update\n",
    "epsilon_start = 0.5\n",
    "epsilon_end = 0.1\n",
    "epsilon_decay = 4_000\n",
    "buffer_size = 100_000 # Maximum number of experiences to store in the replay buffer (after exceeding this limit, the oldest experiences are overwritten with new ones)\n",
    "target_update_interval = 50 # Number of episodes to wait before updating the target network for RL (the one used to compute the target state values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the reward and environment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward function parameters\n",
    "reward_params = {\n",
    "    'goal_reward': 1.,\n",
    "    'collision_penalty': 0.25,\n",
    "    'discomfort_distance': 0.2,\n",
    "    'time_limit': 50.,\n",
    "}\n",
    "\n",
    "# Initialize reward function\n",
    "reward_function = generate_reward_done_function(**reward_params)\n",
    "\n",
    "# Environment parameters\n",
    "env_params = {\n",
    "    'robot_radius': 0.3,\n",
    "    'n_humans': 5, # SARL can be trained with multiple humans\n",
    "    'robot_dt': 0.25,\n",
    "    'humans_dt': 0.01,\n",
    "    'robot_visible': False,\n",
    "    'scenario': 'circular_crossing',\n",
    "    'humans_policy': 'hsfm',\n",
    "    'circle_radius': 7,\n",
    "    'reward_function': reward_function,\n",
    "    'time_limit': reward_params['time_limit'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize environment, robot policy and replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "env = SocialNav(**env_params)\n",
    "\n",
    "# Initialize robot policy and vnet params\n",
    "policy = SARL(env.reward_function, dt=env_params['robot_dt'])\n",
    "initial_vnet_params = policy.model.init(random.key(random_seed), jnp.zeros((env.n_humans, policy.vnet_input_size,)))\n",
    "\n",
    "# Initialize replay buffer\n",
    "replay_buffer = UniformVNetReplayBuffer(buffer_size, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Imitation Learning</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the optimizer and the buffer state dictionary (where experiences will be stored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize IL optimizer\n",
    "optimizer = optax.sgd(learning_rate=il_learning_rate, momentum=0.9)\n",
    "\n",
    "# Initialize buffer state\n",
    "buffer_state = {\n",
    "    'vnet_inputs': jnp.empty((buffer_size, env.n_humans, policy.vnet_input_size)),\n",
    "    'targets': jnp.empty((buffer_size,1)),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set all the parameters for the imitation learning rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "il_rollout_params = {\n",
    "    'initial_vnet_params': initial_vnet_params,\n",
    "    'train_episodes': il_training_episodes,\n",
    "    'random_seed': random_seed,\n",
    "    'optimizer': optimizer,\n",
    "    'buffer_state': buffer_state,\n",
    "    'current_buffer_size': 0,\n",
    "    'policy': policy,\n",
    "    'env': env,\n",
    "    'replay_buffer': replay_buffer,\n",
    "    'buffer_size': buffer_size,\n",
    "    'num_epochs': il_num_epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'success_reward': reward_params['goal_reward'],\n",
    "    'failure_reward': reward_params['collision_penalty']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During imitation learning, the robot will move using the same policy used by humans. Let's start the rollout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating IL episodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running for 3,000 iterations: 100%|██████████| 3000/3000 [00:32<00:00, 91.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer size after IL: 100000\n",
      "Optimizing model on generated experiences for 50 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running for 50 iterations:  32%|███▏      | 16/50 [01:50<04:25,  7.81s/it]"
     ]
    }
   ],
   "source": [
    "il_out = deep_vnet_il_rollout(**il_rollout_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we save the parameters we are interested in from the rollout output and we plot the discounted return over the IL training episodes and the loss over the optimization epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the IL model parameters, buffer state, and keys\n",
    "il_model_params = il_out['model_params']\n",
    "reset_key = il_out['reset_key']\n",
    "policy_key = il_out['policy_key']\n",
    "buffer_state = il_out['buffer_state']\n",
    "current_buffer_size = il_out['current_buffer_size']\n",
    "\n",
    "# Plot the losses and returns\n",
    "window = 100\n",
    "figure, ax = plt.subplots(figsize=(10,10))\n",
    "ax.set(xlabel='Episodes', ylabel='Return', title='Return moving average over {} episodes'.format(window))\n",
    "ax.plot(np.arange(len(il_out['returns'])-(window-1))+window, jnp.convolve(il_out['returns'], jnp.ones(window,), 'valid') / window)\n",
    "plt.show()\n",
    "figure, ax = plt.subplots(figsize=(10,10))\n",
    "ax.set(xlabel='Episodes', ylabel='Loss', title='Loss over {} epochs'.format(len(il_out['losses'])))\n",
    "ax.plot(np.arange(len(il_out['losses'])), il_out['losses'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the IL trained agent on 1000 unseen trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_k_trials(1000, 2, env, policy, il_model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Reinforcement Learning</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the optimizer and the next rollout parameters. We should start from the model parameters compute after IL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RL optimizer\n",
    "optimizer = optax.sgd(learning_rate=rl_learning_rate, momentum=0.9)\n",
    "\n",
    "# Initialize RL rollout params\n",
    "rl_rollout_params = {\n",
    "    'initial_vnet_params': il_model_params,\n",
    "    'train_episodes': rl_training_episodes,\n",
    "    'random_seed': random_seed,\n",
    "    'model': policy.model,\n",
    "    'optimizer': optimizer,\n",
    "    'buffer_state': buffer_state,\n",
    "    'current_buffer_size': current_buffer_size,\n",
    "    'policy': policy,\n",
    "    'env': env,\n",
    "    'replay_buffer': replay_buffer,\n",
    "    'buffer_size': buffer_size,\n",
    "    'num_batches': rl_num_batches,\n",
    "    'epsilon_decay_fn': epsilon_scaling_decay,\n",
    "    'epsilon_start': epsilon_start,\n",
    "    'epsilon_end': epsilon_end,\n",
    "    'decay_rate': epsilon_decay,\n",
    "    'target_update_interval': target_update_interval,\n",
    "    'success_reward': reward_params['goal_reward'],\n",
    "    'failure_reward': reward_params['collision_penalty']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the RL rollout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_out = deep_vnet_rl_rollout(**rl_rollout_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the final model parameters and plot discounted return and loss over the RL training episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model parameters and keys\n",
    "final_model_params = rl_out['model_params']\n",
    "reset_key = rl_out['reset_key']\n",
    "policy_key = rl_out['policy_key']\n",
    "\n",
    "figure, ax = plt.subplots(figsize=(10,10))\n",
    "window = 500\n",
    "ax.plot(np.arange(len(rl_out['losses'])-(window-1))+window, jnp.convolve(rl_out['losses'], jnp.ones(window,), 'valid') / window)\n",
    "ax.set(xlabel='Episodes', ylabel='Loss', title='Loss moving average over {} episodes'.format(window))\n",
    "plt.show()\n",
    "figure, ax = plt.subplots(figsize=(10,10))\n",
    "ax.set(xlabel='Episodes', ylabel='Return', title='Return moving average over {} episodes'.format(window))\n",
    "ax.plot(np.arange(len(rl_out['returns'])-(window-1))+window, jnp.convolve(rl_out['returns'], jnp.ones(window,), 'valid') / window)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the RL trained agent in three environments, with 1, 5 and 10 humans. In all environmentss the robot is NOT visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env10_params = {\n",
    "    'robot_radius': 0.3,\n",
    "    'n_humans': 10,\n",
    "    'robot_dt': 0.25,\n",
    "    'humans_dt': 0.01,\n",
    "    'robot_visible': False,\n",
    "    'scenario': 'circular_crossing',\n",
    "    'humans_policy': 'hsfm',\n",
    "    'circle_radius': 7,\n",
    "    'reward_function': reward_function,\n",
    "    'time_limit': reward_params['time_limit'],\n",
    "}\n",
    "env10 = SocialNav(**env10_params)\n",
    "env15_params = {\n",
    "    'robot_radius': 0.3,\n",
    "    'n_humans': 15,\n",
    "    'robot_dt': 0.25,\n",
    "    'humans_dt': 0.01,\n",
    "    'robot_visible': False,\n",
    "    'scenario': 'circular_crossing',\n",
    "    'humans_policy': 'hsfm',\n",
    "    'circle_radius': 7,\n",
    "    'reward_function': reward_function,\n",
    "    'time_limit': reward_params['time_limit'],\n",
    "}\n",
    "env15 = SocialNav(**env15_params)\n",
    "test_k_trials(1000, 3, env, policy, final_model_params)\n",
    "test_k_trials(1000, 3, env10, policy, final_model_params)\n",
    "test_k_trials(1000, 3, env15, policy, final_model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate some episodes using the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 5\n",
    "env = SocialNav(**env_params)\n",
    "# Simulate some episodes\n",
    "episode_simulation_times = np.empty((n_episodes,))\n",
    "for i in range(n_episodes):\n",
    "    done = False\n",
    "    episode_start_time = time.time()\n",
    "    state, reset_key, obs, info = env.reset(reset_key)\n",
    "    all_states = np.array([state])\n",
    "    while not done:\n",
    "        # action = jnp.array([0.,1.]) # Move north\n",
    "        action, policy_key, _ = policy.act(policy_key, obs, info, final_model_params, 0.)\n",
    "        state, obs, info, reward, done = env.step(state,info,action) \n",
    "        all_states = np.vstack((all_states, [state]))\n",
    "    episode_simulation_times[i] = round(time.time() - episode_start_time,2)\n",
    "    all_states = device_get(all_states) # Transfer data from GPU to CPU for plotting\n",
    "    print(f\"Episode {i} ended - Execution time {episode_simulation_times[i]} seconds - Plotting trajectory...\")\n",
    "    ## Plot episode trajectory\n",
    "    figure, ax = plt.subplots(figsize=(10,10))\n",
    "    ax.axis('equal')\n",
    "    plot_trajectory(ax, all_states, info['humans_goal'], info['robot_goal'])\n",
    "    for k in range(0,len(all_states),int(3/env_params['robot_dt'])):\n",
    "        plot_state(ax, k*env_params['robot_dt'], all_states[k], env_params['humans_policy'], env_params['scenario'], info[\"humans_parameters\"][:,0], env.robot_radius)\n",
    "    # plot last state\n",
    "    plot_state(ax, (len(all_states)-1)*env_params['robot_dt'], all_states[len(all_states)-1], env_params['humans_policy'], env_params['scenario'], info[\"humans_parameters\"][:,0], env.robot_radius)\n",
    "    plt.show()\n",
    "# Print simulation times\n",
    "print(f\"Average time per episode: {round(np.mean(episode_simulation_times),2)} seconds\")\n",
    "print(f\"Total time for {n_episodes} episodes: {round(np.sum(episode_simulation_times),2)} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socialjym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
