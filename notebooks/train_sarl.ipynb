{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>SARL Complete Training (IL + RL)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random, vmap, device_get\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from socialjym.envs.socialnav import SocialNav\n",
    "from socialjym.policies.sarl import SARL\n",
    "from socialjym.utils.replay_buffers.uniform_vnet_replay_buffer import UniformVNetReplayBuffer\n",
    "from socialjym.utils.rollouts.deep_vnet_rollouts import deep_vnet_rl_rollout, deep_vnet_il_rollout\n",
    "from socialjym.utils.aux_functions import epsilon_scaling_decay, plot_state, plot_trajectory, test_k_trials, save_policy_params\n",
    "from socialjym.utils.rewards.reward1 import generate_reward_done_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_hyperparams = {\n",
    "    'random_seed': 1,\n",
    "    'il_training_episodes': 3_000,\n",
    "    'il_learning_rate': 0.001,\n",
    "    'il_num_epochs': 50, # Number of epochs to train the model after ending IL\n",
    "    'rl_training_episodes': 10_000,\n",
    "    'rl_learning_rate': 0.001,\n",
    "    'rl_num_batches': 100, # Number of batches to train the model after each RL episode\n",
    "    'batch_size': 100, # Number of experiences to sample from the replay buffer for each model update\n",
    "    'epsilon_start': 0.5,\n",
    "    'epsilon_end': 0.1,\n",
    "    'epsilon_decay': 4_000,\n",
    "    'buffer_size': 100_000, # Maximum number of experiences to store in the replay buffer (after exceeding this limit, the oldest experiences are overwritten with new ones)\n",
    "    'target_update_interval': 50, # Number of episodes to wait before updating the target network for RL (the one used to compute the target state values)\n",
    "    'humans_policy': 'hsfm',\n",
    "    'scenario': 'hybrid_scenario',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the reward and environment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward function parameters\n",
    "reward_params = {\n",
    "    'goal_reward': 1.,\n",
    "    'collision_penalty': -0.25,\n",
    "    'discomfort_distance': 0.2,\n",
    "    'time_limit': 50.,\n",
    "}\n",
    "\n",
    "# Initialize reward function\n",
    "reward_function = generate_reward_done_function(**reward_params)\n",
    "\n",
    "# Environment parameters\n",
    "env_params = {\n",
    "    'robot_radius': 0.3,\n",
    "    'n_humans': 5, # SARL can be trained with multiple humans\n",
    "    'robot_dt': 0.25,\n",
    "    'humans_dt': 0.01,\n",
    "    'robot_visible': False,\n",
    "    'scenario': training_hyperparams['scenario'],\n",
    "    'humans_policy': training_hyperparams['humans_policy'],\n",
    "    'circle_radius': 7,\n",
    "    'reward_function': reward_function,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize environment, robot policy and replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "env = SocialNav(**env_params)\n",
    "\n",
    "# Initialize robot policy and vnet params\n",
    "policy = SARL(env.reward_function, dt=env_params['robot_dt'])\n",
    "initial_vnet_params = policy.model.init(random.key(training_hyperparams['random_seed']), jnp.zeros((env.n_humans, policy.vnet_input_size,)))\n",
    "\n",
    "# Initialize replay buffer\n",
    "replay_buffer = UniformVNetReplayBuffer(training_hyperparams['buffer_size'], training_hyperparams['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Imitation Learning</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the optimizer and the buffer state dictionary (where experiences will be stored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize IL optimizer\n",
    "optimizer = optax.sgd(learning_rate=training_hyperparams['il_learning_rate'], momentum=0.9)\n",
    "\n",
    "# Initialize buffer state\n",
    "buffer_state = {\n",
    "    'vnet_inputs': jnp.empty((training_hyperparams['buffer_size'], env.n_humans, policy.vnet_input_size)),\n",
    "    'targets': jnp.empty((training_hyperparams['buffer_size'],1)),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set all the parameters for the imitation learning rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "il_rollout_params = {\n",
    "    'initial_vnet_params': initial_vnet_params,\n",
    "    'train_episodes': training_hyperparams['il_training_episodes'],\n",
    "    'random_seed': training_hyperparams['random_seed'],\n",
    "    'optimizer': optimizer,\n",
    "    'buffer_state': buffer_state,\n",
    "    'current_buffer_size': 0,\n",
    "    'policy': policy,\n",
    "    'env': env,\n",
    "    'replay_buffer': replay_buffer,\n",
    "    'buffer_size': training_hyperparams['buffer_size'],\n",
    "    'num_epochs': training_hyperparams['il_num_epochs'],\n",
    "    'batch_size': training_hyperparams['batch_size'],\n",
    "    'time_limit': reward_params['time_limit'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During imitation learning, the robot will move using the same policy used by humans. Let's start the rollout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "il_out = deep_vnet_il_rollout(**il_rollout_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we save the parameters we are interested in from the rollout output and we plot the discounted return over the IL training episodes and the loss over the optimization epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the IL model parameters, buffer state, and keys\n",
    "il_model_params = il_out['model_params']\n",
    "reset_key = il_out['reset_key']\n",
    "policy_key = il_out['policy_key']\n",
    "buffer_state = il_out['buffer_state']\n",
    "current_buffer_size = il_out['current_buffer_size']\n",
    "\n",
    "# Plot the losses and returns\n",
    "window = 100\n",
    "figure, ax = plt.subplots(figsize=(10,10))\n",
    "ax.set(xlabel='Episodes', ylabel='Return', title='Return moving average over {} episodes'.format(window))\n",
    "ax.plot(np.arange(len(il_out['returns'])-(window-1))+window, jnp.convolve(il_out['returns'], jnp.ones(window,), 'valid') / window)\n",
    "plt.show()\n",
    "figure, ax = plt.subplots(figsize=(10,10))\n",
    "ax.set(xlabel='Episodes', ylabel='Loss', title='Loss over {} epochs'.format(len(il_out['losses'])))\n",
    "ax.plot(np.arange(len(il_out['losses'])), il_out['losses'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the IL trained agent on 1000 unseen trials. The robot is still NOT visible by humans here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_k_trials(1000, 2, env, policy, il_model_params, reward_params[\"time_limit\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Reinforcement Learning</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the optimizer and the next rollout parameters. We should start from the model parameters compute after IL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RL optimizer\n",
    "optimizer = optax.sgd(learning_rate=training_hyperparams['rl_learning_rate'], momentum=0.9)\n",
    "\n",
    "# Initialize RL rollout params\n",
    "rl_rollout_params = {\n",
    "    'initial_vnet_params': il_model_params,\n",
    "    'train_episodes': training_hyperparams['rl_training_episodes'],\n",
    "    'random_seed': training_hyperparams['random_seed'],\n",
    "    'model': policy.model,\n",
    "    'optimizer': optimizer,\n",
    "    'buffer_state': buffer_state,\n",
    "    'current_buffer_size': current_buffer_size,\n",
    "    'policy': policy,\n",
    "    'env': env,\n",
    "    'replay_buffer': replay_buffer,\n",
    "    'buffer_size': training_hyperparams['buffer_size'],\n",
    "    'num_batches': training_hyperparams['rl_num_batches'],\n",
    "    'epsilon_decay_fn': epsilon_scaling_decay,\n",
    "    'epsilon_start': training_hyperparams['epsilon_start'],\n",
    "    'epsilon_end': training_hyperparams['epsilon_end'],\n",
    "    'decay_rate': training_hyperparams['epsilon_decay'],\n",
    "    'target_update_interval': training_hyperparams['target_update_interval'],\n",
    "    'time_limit': reward_params['time_limit'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the RL rollout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_out = deep_vnet_rl_rollout(**rl_rollout_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the final model parameters and plot discounted return and loss over the RL training episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model parameters and keys\n",
    "final_model_params = rl_out['model_params']\n",
    "reset_key = rl_out['reset_key']\n",
    "policy_key = rl_out['policy_key']\n",
    "\n",
    "figure, ax = plt.subplots(figsize=(10,10))\n",
    "window = 500\n",
    "ax.plot(np.arange(len(rl_out['losses'])-(window-1))+window, jnp.convolve(rl_out['losses'], jnp.ones(window,), 'valid') / window)\n",
    "ax.set(xlabel='Episodes', ylabel='Loss', title='Loss moving average over {} episodes'.format(window))\n",
    "plt.show()\n",
    "figure, ax = plt.subplots(figsize=(10,10))\n",
    "ax.set(xlabel='Episodes', ylabel='Return', title='Return moving average over {} episodes'.format(window))\n",
    "ax.plot(np.arange(len(rl_out['returns'])-(window-1))+window, jnp.convolve(rl_out['returns'], jnp.ones(window,), 'valid') / window)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the RL trained agent in three environments, with 1, 5 and 10 humans. In all environmentss the robot is NOT visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_params = {\n",
    "    'robot_radius': 0.3,\n",
    "    'n_humans': 5,\n",
    "    'robot_dt': 0.25,\n",
    "    'humans_dt': 0.01,\n",
    "    'robot_visible': True,\n",
    "    'scenario': training_hyperparams['scenario'],\n",
    "    'humans_policy': training_hyperparams['humans_policy'],\n",
    "    'circle_radius': 7,\n",
    "    'reward_function': reward_function,\n",
    "}\n",
    "env = SocialNav(**env_params)\n",
    "env10_params = {\n",
    "    'robot_radius': 0.3,\n",
    "    'n_humans': 10,\n",
    "    'robot_dt': 0.25,\n",
    "    'humans_dt': 0.01,\n",
    "    'robot_visible': True,\n",
    "    'scenario': training_hyperparams['scenario'],\n",
    "    'humans_policy': training_hyperparams['humans_policy'],\n",
    "    'circle_radius': 7,\n",
    "    'reward_function': reward_function,\n",
    "}\n",
    "env10 = SocialNav(**env10_params)\n",
    "env15_params = {\n",
    "    'robot_radius': 0.3,\n",
    "    'n_humans': 15,\n",
    "    'robot_dt': 0.25,\n",
    "    'humans_dt': 0.01,\n",
    "    'robot_visible': True,\n",
    "    'scenario': training_hyperparams['scenario'],\n",
    "    'humans_policy': training_hyperparams['humans_policy'],\n",
    "    'circle_radius': 7,\n",
    "    'reward_function': reward_function,\n",
    "}\n",
    "env15 = SocialNav(**env15_params)\n",
    "test_k_trials(1000, 3, env, policy, final_model_params, reward_params[\"time_limit\"])\n",
    "test_k_trials(1000, 3, env10, policy, final_model_params, reward_params[\"time_limit\"])\n",
    "test_k_trials(1000, 3, env15, policy, final_model_params, reward_params[\"time_limit\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained policy parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_policy_params(\n",
    "    \"sarl\", \n",
    "    final_model_params, \n",
    "    env_params, \n",
    "    reward_params, \n",
    "    training_hyperparams, \n",
    "    os.path.join(os.path.expanduser(\"~\"),\"Repos/social-jym/trained_policies/socialjym_policies/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate some episodes using the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 5\n",
    "env = SocialNav(**env_params)\n",
    "# Simulate some episodes\n",
    "episode_simulation_times = np.empty((n_episodes,))\n",
    "for i in range(n_episodes):\n",
    "    policy_key, reset_key = vmap(random.PRNGKey)(jnp.zeros(2, dtype=int) + i)\n",
    "    outcome = {\"nothing\": True, \"success\": False, \"failure\": False, \"timeout\": False}\n",
    "    episode_start_time = time.time()\n",
    "    state, reset_key, obs, info = env.reset(reset_key)\n",
    "    all_states = np.array([state])\n",
    "    while outcome[\"nothing\"]:\n",
    "        # action = jnp.array([0.,1.]) # Move north\n",
    "        action, policy_key, _ = policy.act(policy_key, obs, info, final_model_params, 0.)\n",
    "        state, obs, info, reward, outcome = env.step(state,info,action,test=True) \n",
    "        all_states = np.vstack((all_states, [state]))\n",
    "    episode_simulation_times[i] = round(time.time() - episode_start_time,2)\n",
    "    all_states = device_get(all_states) # Transfer data from GPU to CPU for plotting\n",
    "    print(f\"Episode {i} ended - Execution time {episode_simulation_times[i]} seconds - Plotting trajectory...\")\n",
    "    ## Plot episode trajectory\n",
    "    figure, ax = plt.subplots(figsize=(10,10))\n",
    "    ax.axis('equal')\n",
    "    plot_trajectory(ax, all_states, info['humans_goal'], info['robot_goal'])\n",
    "    for k in range(0,len(all_states),int(3/env_params['robot_dt'])):\n",
    "        plot_state(ax, k*env_params['robot_dt'], all_states[k], env_params['humans_policy'], info['current_scenario'], info[\"humans_parameters\"][:,0], env.robot_radius)\n",
    "    # plot last state\n",
    "    plot_state(ax, (len(all_states)-1)*env_params['robot_dt'], all_states[len(all_states)-1], env_params['humans_policy'], info['current_scenario'], info[\"humans_parameters\"][:,0], env.robot_radius)\n",
    "    plt.show()\n",
    "# Print simulation times\n",
    "print(f\"Average time per episode: {round(np.mean(episode_simulation_times),2)} seconds\")\n",
    "print(f\"Total time for {n_episodes} episodes: {round(np.sum(episode_simulation_times),2)} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
